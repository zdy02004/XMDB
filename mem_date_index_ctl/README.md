>本模块主要基于C语言实现了基于的内存的存储管理。
* * *
*   底层函数是使用MMAP创建共享内存,并映射到一个大文件，使用mmap的msync来实现checkpoint。
*   一张内存表对应数个大文件，内存超过初始设置值时会自动扩表，申请新的共享内存块。采用传统的行存储模式。
*   每张表表头维护了高水位线，共享内存块起始结束地址，块ID，映射文件路径，块大小等信息。
*   每行行首维护了rowid、锁、mvcc版本号、事务指针、删除标识、回收链表项等信息，用于支持快速定位，空间回收，多版本并发，事务等数据库基本能力。
*   基于内存表实现了用于快速单一查询的hash索引、用于范围查询的跳表索引。
*   实现了一个简单的可持久化的redo机制和基于内存的undo机制。并由此简化recover机制为：只要进行恢复 checkpoint + 重做redo即可。
*   实现了一个简单的元数据管理，后续再考虑优化设计。
*   实现了一个简单的索引实现，后续再考虑优化设计。
* * *
>读写性能
*   大部分数据结构考虑了cache line 对齐，大部分函数基本上内联。
*   锁的粒度基本控制在了行级自旋锁，且被锁保护的区间较短。
*   在一台 24核 2.1Hz， pc server 上,开10线程，进行3KW 次带事务的最简单的增删改的函数级调用，耗时4Min左右，其中1分半左右增删改执行完，后2分半写完redo日志。
*   hash 索引，和跳表索引，使用 自旋锁 +lock free 保证线程安全。虽然无法做到完全的 lock free，但保证锁粒度足够低。
* * *
>刷盘性能
*   由于在共享内存的数据存储中，完全使用逻辑位置代替指针，因次内存数据可无需序列化直接用mmap族函数刷入磁盘，从磁盘刷回内存时，也无需序列化。
因此全量刷盘读盘的速度可以得到保证。后续考虑通过解析元数据的方式实现不同主机的数据移植性。但目前这种实现方式不能跨操作系统和跨大小端。
*   redo日志是先写入到一个无锁队列中，再由专门的线程写入磁盘。虽然未使用libaio 。写入速度仍然不差。
* * *
>存储效率
*   目前这种重内存的实现方式，必然会影响存储效率，后续会考虑从设计上优化！
* * *
>稳定性
*   主要功能都写有test验证。但由于是个人业余项目，目前缺少足够的时间测试。
